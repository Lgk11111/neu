import os
import cv2
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
import warnings

warnings.filterwarnings('ignore')

# ===================== 1. é…ç½®å‚æ•°ï¼ˆå…³é”®ä¿®æ”¹ï¼šé¢„æµ‹ä¿å­˜è·¯å¾„æ”¹ä¸º./imageï¼‰ =====================
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
TRAIN_IMG_DIR = "train/image"
TRAIN_LABEL_DIR = "train/label"
TEST_IMG_DIR = "test/image"
PRED_SAVE_DIR = "./image"  # æ ¸å¿ƒä¿®æ”¹ï¼šé€‚é…segmentation_to_csv.pyçš„imageç›®å½•
BATCH_SIZE = 4
EPOCHS = 50
LEARNING_RATE = 1e-4
IMG_SIZE = 256  # è¾“å…¥ç½‘ç»œçš„å°ºå¯¸ï¼Œæœ€ç»ˆè¾“å‡ºè¿˜åŸä¸º565x584


# ===================== 2. è‡ªå®šä¹‰æ•°æ®é›†ï¼ˆå®Œå…¨ä¿ç•™ä½ çš„åŸä»£ç ï¼‰ =====================
class FundusDataset(Dataset):
    def __init__(self, img_dir, label_dir=None, train_mode=True):
        self.img_dir = img_dir
        self.label_dir = label_dir
        self.train_mode = train_mode
        self.img_names = sorted(
            [f for f in os.listdir(img_dir) if f.lower().endswith('.jpg')],
            key=lambda x: int(os.path.splitext(x)[0])
        )
        # åŸå§‹å°ºå¯¸è®°å½•ï¼ˆç”¨äºæµ‹è¯•é›†è¿˜åŸï¼‰
        self.raw_sizes = {}
        for img_name in self.img_names:
            img_path = os.path.join(img_dir, img_name)
            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
            if img is not None:
                self.raw_sizes[img_name] = (img.shape[1], img.shape[0])  # (w, h)

        # è®­ç»ƒé›†æ•°æ®å¢å¼º
        self.train_transform = transforms.Compose([
            transforms.ToPILImage(),
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.RandomVerticalFlip(p=0.2),
            transforms.RandomRotation(5),
            transforms.Resize((IMG_SIZE, IMG_SIZE)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.5], std=[0.5])
        ])

        # æµ‹è¯•é›†ä»…åšå½’ä¸€åŒ–å’Œç¼©æ”¾
        self.test_transform = transforms.Compose([
            transforms.ToPILImage(),
            transforms.Resize((IMG_SIZE, IMG_SIZE)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.5], std=[0.5])
        ])

    def __len__(self):
        return len(self.img_names)

    def __getitem__(self, idx):
        img_name = self.img_names[idx]
        img_path = os.path.join(self.img_dir, img_name)

        # è¯»å–ç°åº¦å›¾
        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
        img = np.expand_dims(img, axis=-1)  # (H, W, 1)

        if self.train_mode and self.label_dir is not None:
            # è¯»å–æ ‡ç­¾ï¼ˆè¡€ç®¡=0ï¼ŒèƒŒæ™¯=255ï¼‰
            label_path = os.path.join(self.label_dir, img_name)
            label = cv2.imread(label_path, cv2.IMREAD_GRAYSCALE)

            # æ ‡ç­¾é¢„å¤„ç†ï¼šè¡€ç®¡=1ï¼ŒèƒŒæ™¯=0ï¼ˆé€‚é…ç½‘ç»œè®­ç»ƒï¼‰
            label = (label == 0).astype(np.float32)  # è¡€ç®¡åŒºåŸŸè½¬ä¸º1

            # æ•°æ®å¢å¼ºï¼ˆåŒæ­¥åº”ç”¨åˆ°å›¾åƒå’Œæ ‡ç­¾ï¼‰
            seed = np.random.randint(2147483647)
            torch.manual_seed(seed)
            img = self.train_transform(img)
            torch.manual_seed(seed)
            label = self.train_transform(label)
            label = label.squeeze(0)  # (1, H, W) â†’ (H, W)

            return img, label
        else:
            # æµ‹è¯•é›†ä»…è¿”å›å›¾åƒã€åç§°ã€åŸå§‹å°ºå¯¸ï¼ˆä¿®å¤ï¼šè¿”å›tupleè€Œétensorï¼‰
            img = self.test_transform(img)
            w, h = self.raw_sizes[img_name]
            return img, img_name, (w, h)  # å…³é”®ä¿®å¤ï¼šç›´æ¥è¿”å›tuple


# ===================== 3. U-Netç½‘ç»œï¼ˆå®Œå…¨ä¿ç•™ä½ çš„åŸä»£ç ï¼‰ =====================
class DoubleConv(nn.Module):
    def __init__(self, in_ch, out_ch):
        super(DoubleConv, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(in_ch, out_ch, 3, padding=1),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_ch, out_ch, 3, padding=1),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True)
        )

    def forward(self, x):
        return self.conv(x)


class Down(nn.Module):
    def __init__(self, in_ch, out_ch):
        super(Down, self).__init__()
        self.mpconv = nn.Sequential(
            nn.MaxPool2d(2),
            DoubleConv(in_ch, out_ch)
        )

    def forward(self, x):
        return self.mpconv(x)


class Up(nn.Module):
    def __init__(self, in_ch, out_ch, bilinear=True):
        super(Up, self).__init__()
        if bilinear:
            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)
        else:
            self.up = nn.ConvTranspose2d(in_ch // 2, in_ch // 2, 2, stride=2)
        self.conv = DoubleConv(in_ch, out_ch)

    def forward(self, x1, x2):
        x1 = self.up(x1)
        diffY = x2.size()[2] - x1.size()[2]
        diffX = x2.size()[3] - x1.size()[3]
        x1 = nn.functional.pad(x1, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])
        x = torch.cat([x2, x1], dim=1)
        return self.conv(x)


class OutConv(nn.Module):
    def __init__(self, in_ch, out_ch):
        super(OutConv, self).__init__()
        self.conv = nn.Conv2d(in_ch, out_ch, 1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        return self.sigmoid(self.conv(x))


class UNet(nn.Module):
    def __init__(self, n_channels=1, n_classes=1):
        super(UNet, self).__init__()
        self.inc = DoubleConv(n_channels, 64)
        self.down1 = Down(64, 128)
        self.down2 = Down(128, 256)
        self.down3 = Down(256, 512)
        self.down4 = Down(512, 512)
        self.up1 = Up(1024, 256)
        self.up2 = Up(512, 128)
        self.up3 = Up(256, 64)
        self.up4 = Up(128, 64)
        self.outc = OutConv(64, n_classes)

    def forward(self, x):
        x1 = self.inc(x)
        x2 = self.down1(x1)
        x3 = self.down2(x2)
        x4 = self.down3(x3)
        x5 = self.down4(x4)
        x = self.up1(x5, x4)
        x = self.up2(x, x3)
        x = self.up3(x, x2)
        x = self.up4(x, x1)
        logits = self.outc(x)
        return logits


# ===================== 4. DiceæŸå¤±å‡½æ•°ï¼ˆå®Œå…¨ä¿ç•™ä½ çš„åŸä»£ç ï¼‰ =====================
class DiceLoss(nn.Module):
    def __init__(self, smooth=1e-6):
        super(DiceLoss, self).__init__()
        self.smooth = smooth

    def forward(self, pred, target):
        pred = pred.view(-1)
        target = target.view(-1)
        intersection = (pred * target).sum()
        dice = (2. * intersection + self.smooth) / (pred.sum() + target.sum() + self.smooth)
        return 1 - dice


# ===================== 5. è®­ç»ƒå‡½æ•°ï¼ˆå®Œå…¨ä¿ç•™ä½ çš„åŸä»£ç ï¼‰ =====================
def train_model():
    # åŠ è½½è®­ç»ƒé›†
    train_dataset = FundusDataset(TRAIN_IMG_DIR, TRAIN_LABEL_DIR, train_mode=True)
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)

    # åˆå§‹åŒ–æ¨¡å‹ã€æŸå¤±ã€ä¼˜åŒ–å™¨
    model = UNet(n_channels=1, n_classes=1).to(DEVICE)
    criterion = DiceLoss()
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.8)

    print(f"===== å¼€å§‹è®­ç»ƒï¼ˆ{DEVICE}ï¼‰ =====")
    print(f"è®­ç»ƒé›†æ•°é‡ï¼š{len(train_dataset)}")
    print(f"æ€»è½®æ•°ï¼š{EPOCHS} | æ‰¹æ¬¡å¤§å°ï¼š{BATCH_SIZE}")

    # è®­ç»ƒå¾ªç¯
    best_loss = float('inf')
    for epoch in range(EPOCHS):
        model.train()
        total_loss = 0.0

        for batch_idx, (imgs, labels) in enumerate(train_loader):
            imgs = imgs.to(DEVICE)
            labels = labels.to(DEVICE)

            # å‰å‘ä¼ æ’­
            outputs = model(imgs)
            loss = criterion(outputs.squeeze(1), labels)

            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        # å­¦ä¹ ç‡è°ƒæ•´
        scheduler.step()

        # æ‰“å°è®­ç»ƒä¿¡æ¯
        avg_loss = total_loss / len(train_loader)
        print(f"Epoch [{epoch + 1}/{EPOCHS}] | å¹³å‡æŸå¤±ï¼š{avg_loss:.4f} | å­¦ä¹ ç‡ï¼š{scheduler.get_last_lr()[0]:.6f}")

        # ä¿å­˜æœ€ä¼˜æ¨¡å‹
        if avg_loss < best_loss:
            best_loss = avg_loss
            torch.save(model.state_dict(), "best_unet.pth")
            print(f"âœ… ä¿å­˜æœ€ä¼˜æ¨¡å‹ï¼ˆæŸå¤±ï¼š{best_loss:.4f}ï¼‰")

    print("===== è®­ç»ƒå®Œæˆ =====")
    return model


# ===================== 6. æµ‹è¯•é›†é¢„æµ‹ï¼ˆä»…ä¿®æ”¹ä¿å­˜è·¯å¾„ï¼Œå…¶ä½™ä¿ç•™ï¼‰ =====================
def predict_test_set(model):
    # åŠ è½½æµ‹è¯•é›†ï¼ˆå…³é”®ä¿®å¤ï¼šcollate_fné¿å…tupleè¢«è½¬ä¸ºtensorï¼‰
    def collate_fn(batch):
        imgs = torch.stack([item[0] for item in batch])
        img_names = [item[1] for item in batch]
        raw_sizes = [item[2] for item in batch]
        return imgs, img_names, raw_sizes

    test_dataset = FundusDataset(TEST_IMG_DIR, label_dir=None, train_mode=False)
    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)

    # åŠ è½½æœ€ä¼˜æƒé‡
    model.load_state_dict(torch.load("best_unet.pth", map_location=DEVICE))
    model.eval()

    # åˆ›å»ºä¿å­˜ç›®å½•ï¼ˆæ”¹ä¸º./imageï¼Œé€‚é…CSVè„šæœ¬ï¼‰
    os.makedirs(PRED_SAVE_DIR, exist_ok=True)
    # æ¸…ç©ºæ—§æ–‡ä»¶ï¼ˆé˜²æ­¢é‡å¤ï¼‰
    for f in os.listdir(PRED_SAVE_DIR):
        if f.lower().endswith('.jpg'):
            os.remove(os.path.join(PRED_SAVE_DIR, f))

    print("\n===== å¼€å§‹æµ‹è¯•é›†é¢„æµ‹ =====")
    with torch.no_grad():
        for imgs, img_names, raw_sizes in test_loader:
            imgs = imgs.to(DEVICE)
            outputs = model(imgs)

            # è½¬æ¢ä¸ºåˆ†å‰²ç»“æœï¼ˆè¡€ç®¡=0ï¼ŒèƒŒæ™¯=255ï¼‰
            pred = outputs.squeeze().cpu().numpy()
            pred = (pred > 0.5).astype(np.uint8)  # é¢„æµ‹ä¸ºè¡€ç®¡çš„åŒºåŸŸ=1
            pred = 255 - (pred * 255)  # åè½¬ï¼šè¡€ç®¡=0ï¼ŒèƒŒæ™¯=255

            # è¿˜åŸä¸ºåŸå§‹å°ºå¯¸ï¼ˆ565x584ï¼‰- å…³é”®ä¿®å¤ï¼šå–listä¸­çš„tuple
            w, h = raw_sizes[0]
            pred = cv2.resize(pred, (w, h), interpolation=cv2.INTER_LINEAR)

            # ä¿å­˜ç»“æœåˆ°./imageç›®å½•ï¼ˆé€‚é…CSVè„šæœ¬ï¼‰
            img_name = img_names[0]
            save_path = os.path.join(PRED_SAVE_DIR, img_name)
            cv2.imwrite(save_path, pred)
            print(f"å·²ä¿å­˜ï¼š{save_path} | åŸå§‹å°ºå¯¸ï¼š{w}x{h}")

    print("===== æµ‹è¯•é›†é¢„æµ‹å®Œæˆ =====")


# ===================== 7. ç”Ÿæˆæäº¤æ–‡ä»¶ï¼ˆä»…ä¼˜åŒ–è°ƒç”¨é€»è¾‘ï¼Œä¿ç•™åŸè„šæœ¬ï¼‰ =====================
def generate_submission():
    print("\n===== ç”Ÿæˆæäº¤æ–‡ä»¶ =====")
    try:
        # æ£€æŸ¥imageç›®å½•æ˜¯å¦æœ‰æ–‡ä»¶
        if not os.listdir(PRED_SAVE_DIR):
            raise ValueError(f"{PRED_SAVE_DIR}ç›®å½•ä¸ºç©ºï¼é¢„æµ‹æœªç”Ÿæˆåˆ†å‰²å›¾")

        # è°ƒç”¨å¤–éƒ¨CSVè„šæœ¬ï¼ˆä¿ç•™ä½ çš„åŸé€»è¾‘ï¼Œå¢åŠ è·¯å¾„æ£€æŸ¥ï¼‰
        os.system("python segmentation_to_csv.py")

        # éªŒè¯CSVæ˜¯å¦ç”Ÿæˆ
        if os.path.exists("submission.csv"):
            df = pd.read_csv("submission.csv")  # ä¸´æ—¶å¯¼å…¥pandasåšéªŒè¯
            print(f"âœ… æäº¤æ–‡ä»¶ç”ŸæˆæˆåŠŸï¼šsubmission.csv")
            print(f"   - æ•°æ®è¡Œæ•°ï¼š{len(df)}è¡Œï¼ˆå«æ ‡é¢˜å…±{len(df) + 1}è¡Œï¼‰")
            print(f"   - IdèŒƒå›´ï¼š{df['Id'].min()} ~ {df['Id'].max()}")
        else:
            raise FileNotFoundError("submission.csvæœªç”Ÿæˆ")

    except ImportError:
        print("âš ï¸  éªŒè¯CSVéœ€å®‰è£…pandasï¼špip install pandasï¼ˆä¸å½±å“CSVç”Ÿæˆï¼‰")
        print("âœ… æäº¤æ–‡ä»¶å·²è°ƒç”¨segmentation_to_csv.pyç”Ÿæˆ")
    except Exception as e:
        print(f"âš ï¸  ç”ŸæˆCSVå¤±è´¥ï¼š{str(e)}")
        print("ğŸ’¡ æ’æŸ¥å»ºè®®ï¼š")
        print(f"   1. æ£€æŸ¥{PRED_SAVE_DIR}ç›®å½•æ˜¯å¦æœ‰1~20.jpg")
        print("   2. ç¡®ä¿segmentation_to_csv.pyåœ¨å½“å‰ç›®å½•")
        print("   3. è¿è¡Œå‰å…³é—­å·²æ‰“å¼€çš„submission.csv")


# ===================== 8. ä¸»å‡½æ•°ï¼ˆä¸€é”®è¿è¡Œï¼‰ =====================
if __name__ == "__main__":
    # æå‰å¯¼å…¥pandasï¼ˆä»…ç”¨äºCSVéªŒè¯ï¼Œéå¿…éœ€ï¼‰
    try:
        import pandas as pd
    except ImportError:
        print("âš ï¸  æœªå®‰è£…pandasï¼Œä»…å½±å“CSVéªŒè¯ï¼Œä¸å½±å“è®­ç»ƒ/é¢„æµ‹")

    # æ­¥éª¤1ï¼šè®­ç»ƒæ¨¡å‹
    model = train_model()

    # æ­¥éª¤2ï¼šæµ‹è¯•é›†é¢„æµ‹ï¼ˆç»“æœä¿å­˜åˆ°./imageï¼‰
    predict_test_set(model)

    # æ­¥éª¤3ï¼šè°ƒç”¨å¤–éƒ¨è„šæœ¬ç”ŸæˆCSV
    generate_submission()

    print("\n===== å…¨éƒ¨æµç¨‹å®Œæˆ =====\nâœ… è®­ç»ƒï¼šbest_unet.pth\nâœ… åˆ†å‰²ç»“æœï¼š./image\nâœ… æäº¤æ–‡ä»¶ï¼šsubmission.csv")