"""
æ¤ç‰©å›¾åƒåˆ†ç±»ç³»ç»Ÿ - è¯¾ç¨‹ä¼˜åŒ–æœ€ç»ˆç‰ˆï¼ˆçº¯æœºå™¨å­¦ä¹ ï¼‰
ç›®æ ‡åˆ†æ•°ï¼š0.80+
ç¬¦åˆè¯¾ç¨‹è¦æ±‚ï¼šç‰¹å¾å·¥ç¨‹ + ä¼ ç»Ÿæœºå™¨å­¦ä¹ 
"""
import os
import cv2
import numpy as np
import pandas as pd
import joblib
import time
from collections import Counter
import warnings

warnings.filterwarnings('ignore')

# sklearnç›¸å…³
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, BaggingClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis
from sklearn.metrics import accuracy_score, classification_report
from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectKBest, f_classif, RFE
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier


# ==================== é…ç½®å‚æ•° ====================
class Config:
    BASE_DIR = os.path.dirname(os.path.abspath(__file__))
    TRAIN_DATA_PATH = os.path.join(BASE_DIR, "data", "train")
    TEST_DATA_PATH = os.path.join(BASE_DIR, "data", "test")
    IMAGE_SIZE = (128, 128)  # å¹³è¡¡ç‰¹å¾æå–é€Ÿåº¦å’Œä¿¡æ¯ä¿ç•™
    MODEL_SAVE_PATH = os.path.join(BASE_DIR, "plant_classifier_final.pkl")
    SUBMISSION_PATH = os.path.join(BASE_DIR, "submission_final.csv")
    RANDOM_STATE = 42
    TEST_SIZE = 0.2

    # ç‰¹å¾æå–å¢å¼º
    USE_COLOR_MOMENTS = True
    USE_GLCM = True
    USE_HOG = True
    USE_LBP = True
    USE_SHAPE = True
    USE_GRADIENT = True

    # æ¨¡å‹é…ç½®
    ENSEMBLE_METHOD = 'voting'  # 'voting' æˆ– 'stacking'
    USE_FEATURE_SELECTION = True
    N_BEST_FEATURES = 150


# ==================== é«˜çº§ç‰¹å¾æå–å‡½æ•° ====================
def extract_color_features_enhanced(image):
    """å¢å¼ºç‰ˆé¢œè‰²ç‰¹å¾æå–"""
    features = []

    # 1. å¤šé¢œè‰²ç©ºé—´ç›´æ–¹å›¾
    color_spaces = {
        'BGR': image,
        'HSV': cv2.cvtColor(image, cv2.COLOR_BGR2HSV),
        'LAB': cv2.cvtColor(image, cv2.COLOR_BGR2LAB),
        'YCrCb': cv2.cvtColor(image, cv2.COLOR_BGR2YCrCb)
    }

    for space_name, space_img in color_spaces.items():
        for channel in range(3):
            hist = cv2.calcHist([space_img], [channel], None, [16], [0, 256])
            cv2.normalize(hist, hist)
            features.extend(hist.flatten())

    # 2. é¢œè‰²çŸ©ï¼ˆå‡å€¼ã€æ ‡å‡†å·®ã€ååº¦ã€å³°åº¦ï¼‰
    for channel in range(3):
        channel_data = image[:, :, channel].flatten()

        # å‡å€¼
        mean_val = np.mean(channel_data)
        features.append(mean_val)

        # æ ‡å‡†å·®
        std_val = np.std(channel_data)
        features.append(std_val)

        # ååº¦ï¼ˆå®‰å…¨è®¡ç®—ï¼‰
        if std_val > 0:
            skewness = np.mean(((channel_data - mean_val) / std_val) ** 3)
        else:
            skewness = 0
        features.append(skewness)

        # å³°åº¦
        if std_val > 0:
            kurtosis = np.mean(((channel_data - mean_val) / std_val) ** 4) - 3
        else:
            kurtosis = 0
        features.append(kurtosis)

    # 3. é¢œè‰²ç›¸å…³æ€§ç‰¹å¾
    # è®¡ç®—é¢œè‰²é€šé“é—´çš„ç›¸å…³ç³»æ•°
    for i in range(3):
        for j in range(i + 1, 3):
            corr = np.corrcoef(image[:, :, i].flatten(), image[:, :, j].flatten())[0, 1]
            features.append(corr if not np.isnan(corr) else 0)

    return np.array(features)


def extract_texture_features_enhanced(image):
    """å¢å¼ºç‰ˆçº¹ç†ç‰¹å¾æå–"""
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    features = []

    # 1. GLCMç‰¹å¾ï¼ˆç°åº¦å…±ç”ŸçŸ©é˜µï¼‰
    # è®¡ç®—ä¸åŒè·ç¦»å’Œæ–¹å‘çš„GLCM
    distances = [1, 3, 5]
    angles = [0, np.pi / 4, np.pi / 2, 3 * np.pi / 4]

    for d in distances:
        for a in angles:
            # ç®€åŒ–çš„GLCMç‰¹å¾è®¡ç®—
            rows, cols = gray.shape
            glcm = np.zeros((256, 256), dtype=np.float32)

            for i in range(rows - d):
                for j in range(cols - d):
                    p1 = gray[i, j]
                    p2 = gray[i + int(d * np.sin(a)), j + int(d * np.cos(a))]
                    glcm[p1, p2] += 1

            if glcm.sum() > 0:
                glcm /= glcm.sum()

                # å¯¹æ¯”åº¦
                i_idx, j_idx = np.indices(glcm.shape)
                contrast = np.sum(glcm * ((i_idx - j_idx) ** 2))
                features.append(contrast)

                # èƒ½é‡
                energy = np.sum(glcm ** 2)
                features.append(energy)

                # åŒè´¨æ€§
                homogeneity = np.sum(glcm / (1 + (i_idx - j_idx) ** 2))
                features.append(homogeneity)
            else:
                features.extend([0, 0, 0])

    # 2. LBPç‰¹å¾ï¼ˆå±€éƒ¨äºŒå€¼æ¨¡å¼ï¼‰
    radius = 1
    n_points = 8 * radius

    height, width = gray.shape
    lbp = np.zeros_like(gray)

    for i in range(radius, height - radius):
        for j in range(radius, width - radius):
            center = gray[i, j]
            code = 0
            for k in range(n_points):
                theta = 2 * np.pi * k / n_points
                x = int(i + radius * np.cos(theta))
                y = int(j + radius * np.sin(theta))
                if gray[x, y] >= center:
                    code |= 1 << k
            lbp[i, j] = code

    # LBPç›´æ–¹å›¾
    hist, _ = np.histogram(lbp.ravel(), bins=256, range=(0, 256))
    hist = hist.astype("float")
    hist /= (hist.sum() + 1e-6)
    features.extend(hist[:32])  # åªå–å‰32ä¸ªbin

    # 3. Tamuraçº¹ç†ç‰¹å¾ï¼ˆç®€åŒ–ç‰ˆï¼‰
    # ç²—ç³™åº¦
    kernel_sizes = [3, 5, 7]
    for ksize in kernel_sizes:
        blurred = cv2.GaussianBlur(gray, (ksize, ksize), 0)
        diff = cv2.absdiff(gray, blurred)
        features.append(np.mean(diff))
        features.append(np.std(diff))

    return np.array(features)


def extract_shape_features_enhanced(image):
    """å¢å¼ºç‰ˆå½¢çŠ¶ç‰¹å¾æå–"""
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    features = []

    # 1. è¾¹ç¼˜æ£€æµ‹
    edges = cv2.Canny(gray, 50, 150)

    # 2. è½®å»“ç‰¹å¾
    contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    if contours:
        # æ‰¾åˆ°æœ€å¤§è½®å»“
        main_contour = max(contours, key=cv2.contourArea)

        # åŸºæœ¬å½¢çŠ¶ç‰¹å¾
        area = cv2.contourArea(main_contour)
        perimeter = cv2.arcLength(main_contour, True)

        features.append(area)
        features.append(perimeter)

        # å½¢çŠ¶æè¿°ç¬¦
        if perimeter > 0:
            # åœ†å½¢åº¦
            circularity = 4 * np.pi * area / (perimeter ** 2)
            features.append(circularity)

            # ç´§å¯†åº¦
            compactness = area / (perimeter ** 2)
            features.append(compactness)
        else:
            features.extend([0, 0])

        # çŸ©å½¢åº¦
        x, y, w, h = cv2.boundingRect(main_contour)
        rect_area = w * h
        if rect_area > 0:
            rectangularity = area / rect_area
            features.append(rectangularity)
        else:
            features.append(0)

        # çºµæ¨ªæ¯”
        if h > 0:
            aspect_ratio = w / h
            features.append(aspect_ratio)
        else:
            features.append(0)

        # HuçŸ©ï¼ˆ7ä¸ªä¸å˜çŸ©ï¼‰
        moments = cv2.moments(main_contour)
        if moments['m00'] > 0:
            hu_moments = cv2.HuMoments(moments).flatten()
            # å–å¯¹æ•°å‹ç¼©èŒƒå›´
            hu_moments = -np.sign(hu_moments) * np.log10(np.abs(hu_moments) + 1e-10)
            features.extend(hu_moments)
        else:
            features.extend([0] * 7)
    else:
        features = [0] * 13  # 13ä¸ªå½¢çŠ¶ç‰¹å¾

    # 3. å‡¸åŒ…ç‰¹å¾
    if contours:
        hull = cv2.convexHull(main_contour)
        hull_area = cv2.contourArea(hull)
        if hull_area > 0:
            solidity = area / hull_area
            features.append(solidity)
        else:
            features.append(0)
    else:
        features.append(0)

    return np.array(features)


def extract_hog_features_enhanced(image):
    """å¢å¼ºç‰ˆHOGç‰¹å¾"""
    if len(image.shape) == 3:
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    else:
        gray = image

    # è°ƒæ•´å¤§å°ä»¥æ ‡å‡†åŒ–ç‰¹å¾ç»´åº¦
    gray = cv2.resize(gray, (64, 64))

    # è®¡ç®—æ¢¯åº¦
    gx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)
    gy = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)

    # è®¡ç®—æ¢¯åº¦çš„å¹…åº¦å’Œæ–¹å‘
    magnitude, angle = cv2.cartToPolar(gx, gy, angleInDegrees=True)

    # è®¡ç®—HOGç‰¹å¾
    cell_size = 8
    bin_count = 9
    features = []

    for i in range(0, gray.shape[0], cell_size):
        for j in range(0, gray.shape[1], cell_size):
            cell_mag = magnitude[i:i + cell_size, j:j + cell_size]
            cell_angle = angle[i:i + cell_size, j:j + cell_size]

            # è®¡ç®—æ–¹å‘ç›´æ–¹å›¾
            hist, _ = np.histogram(cell_angle, bins=bin_count, range=(0, 180), weights=cell_mag)

            # å½’ä¸€åŒ–
            hist = hist.astype("float")
            hist /= (hist.sum() + 1e-6)
            features.extend(hist)

    # æ·»åŠ æ¢¯åº¦ç»Ÿè®¡ç‰¹å¾
    features.append(np.mean(magnitude))
    features.append(np.std(magnitude))
    features.append(np.max(magnitude))

    return np.array(features)


def extract_gradient_features(image):
    """æ¢¯åº¦ç‰¹å¾"""
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    features = []

    # Sobelç®—å­
    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)
    sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)

    # æ¢¯åº¦å¹…åº¦
    grad_mag = np.sqrt(sobelx ** 2 + sobely ** 2)

    # æ¢¯åº¦æ–¹å‘
    grad_dir = np.arctan2(sobely, sobelx)

    # ç»Ÿè®¡ç‰¹å¾
    features.append(np.mean(grad_mag))
    features.append(np.std(grad_mag))
    features.append(np.max(grad_mag))
    features.append(np.min(grad_mag))

    # æ–¹å‘ç›´æ–¹å›¾
    dir_hist, _ = np.histogram(grad_dir.ravel(), bins=8, range=(-np.pi, np.pi))
    dir_hist = dir_hist.astype("float")
    dir_hist /= (dir_hist.sum() + 1e-6)
    features.extend(dir_hist)

    return np.array(features)


def extract_all_features_final(image_path, config):
    """æœ€ç»ˆç‰ˆç‰¹å¾æå–"""
    img = cv2.imread(image_path)
    if img is None:
        print(f"è­¦å‘Š: æ— æ³•è¯»å–å›¾åƒ {image_path}")
        return None

    # è°ƒæ•´å¤§å°
    img = cv2.resize(img, config.IMAGE_SIZE)

    features = []

    # æå–å„ç§ç‰¹å¾
    if config.USE_COLOR_MOMENTS:
        color_features = extract_color_features_enhanced(img)
        features.extend(color_features)

    if config.USE_GLCM or config.USE_LBP:
        texture_features = extract_texture_features_enhanced(img)
        features.extend(texture_features)

    if config.USE_SHAPE:
        shape_features = extract_shape_features_enhanced(img)
        features.extend(shape_features)

    if config.USE_HOG:
        hog_features = extract_hog_features_enhanced(img)
        features.extend(hog_features)

    if config.USE_GRADIENT:
        gradient_features = extract_gradient_features(img)
        features.extend(gradient_features)

    # è½¬æ¢ä¸ºnumpyæ•°ç»„å¹¶å¤„ç†å¼‚å¸¸å€¼
    features_array = np.array(features, dtype=np.float32)
    features_array = np.nan_to_num(features_array, nan=0.0, posinf=0.0, neginf=0.0)

    return features_array


# ==================== æ•°æ®å¢å¼º ====================
def augment_image_simple(image):
    """ç®€å•æ•°æ®å¢å¼º"""
    augmented_images = []

    # åŸå§‹å›¾åƒ
    augmented_images.append(image)

    # æ°´å¹³ç¿»è½¬
    augmented_images.append(cv2.flip(image, 1))

    # å‚ç›´ç¿»è½¬
    augmented_images.append(cv2.flip(image, 0))

    # æ—‹è½¬ï¼ˆå°è§’åº¦ï¼‰
    rows, cols = image.shape[:2]
    for angle in [10, -10]:
        M = cv2.getRotationMatrix2D((cols / 2, rows / 2), angle, 1)
        rotated = cv2.warpAffine(image, M, (cols, rows))
        augmented_images.append(rotated)

    return augmented_images


# ==================== æ¨¡å‹æ„å»º ====================
def create_advanced_ensemble(n_features, n_classes, config):
    """åˆ›å»ºé«˜çº§é›†æˆæ¨¡å‹"""

    if config.ENSEMBLE_METHOD == 'stacking':
        # Stackingé›†æˆ
        from sklearn.ensemble import StackingClassifier

        # ç¬¬ä¸€å±‚ï¼šåŸºç¡€æ¨¡å‹
        base_models = [
            ('rf1', RandomForestClassifier(
                n_estimators=200, max_depth=20,
                min_samples_split=5, min_samples_leaf=2,
                random_state=config.RANDOM_STATE,
                class_weight='balanced',
                n_jobs=-1
            )),
            ('rf2', RandomForestClassifier(
                n_estimators=200, max_depth=15,
                min_samples_split=10, min_samples_leaf=4,
                random_state=config.RANDOM_STATE + 1,
                class_weight='balanced',
                n_jobs=-1
            )),
            ('gb1', GradientBoostingClassifier(
                n_estimators=150, learning_rate=0.1,
                max_depth=6, subsample=0.8,
                random_state=config.RANDOM_STATE
            )),
            ('gb2', GradientBoostingClassifier(
                n_estimators=150, learning_rate=0.05,
                max_depth=8, subsample=0.7,
                random_state=config.RANDOM_STATE + 1
            )),
            ('svm1', SVC(
                C=10, kernel='rbf', gamma='scale',
                probability=True,
                random_state=config.RANDOM_STATE
            )),
            ('svm2', SVC(
                C=5, kernel='poly', degree=3,
                probability=True,
                random_state=config.RANDOM_STATE + 1
            )),
            ('knn', KNeighborsClassifier(
                n_neighbors=7, weights='distance',
                metric='minkowski', p=2,
                n_jobs=-1
            )),
            ('lda', LinearDiscriminantAnalysis())
        ]

        # ç¬¬äºŒå±‚ï¼šå…ƒå­¦ä¹ å™¨
        meta_learner = LogisticRegression(
            C=1.0, solver='lbfgs',
            multi_class='multinomial',
            max_iter=2000,
            random_state=config.RANDOM_STATE
        )

        model = StackingClassifier(
            estimators=base_models,
            final_estimator=meta_learner,
            cv=5,
            passthrough=False,
            n_jobs=-1
        )

    else:  # votingé›†æˆ
        # åˆ›å»ºå¤šæ ·åŒ–çš„åŸºç¡€æ¨¡å‹
        rf1 = RandomForestClassifier(
            n_estimators=300, max_depth=20,
            min_samples_split=5, min_samples_leaf=2,
            random_state=config.RANDOM_STATE,
            class_weight='balanced',
            n_jobs=-1
        )

        rf2 = RandomForestClassifier(
            n_estimators=300, max_depth=15,
            min_samples_split=10, min_samples_leaf=4,
            random_state=config.RANDOM_STATE + 1,
            class_weight='balanced',
            n_jobs=-1
        )

        gb = GradientBoostingClassifier(
            n_estimators=200, learning_rate=0.1,
            max_depth=6, subsample=0.8,
            random_state=config.RANDOM_STATE
        )

        svm = SVC(
            C=10, kernel='rbf', gamma='scale',
            probability=True,
            random_state=config.RANDOM_STATE
        )

        knn = KNeighborsClassifier(
            n_neighbors=9, weights='distance',
            metric='minkowski', p=2,
            n_jobs=-1
        )

        lda = LinearDiscriminantAnalysis()

        qda = QuadraticDiscriminantAnalysis()

        # Baggingå¢å¼ºçš„å†³ç­–æ ‘
        bagging_dt = BaggingClassifier(
            base_estimator=DecisionTreeClassifier(
                max_depth=10,
                random_state=config.RANDOM_STATE
            ),
            n_estimators=50,
            random_state=config.RANDOM_STATE,
            n_jobs=-1
        )

        model = VotingClassifier(
            estimators=[
                ('rf1', rf1),
                ('rf2', rf2),
                ('gb', gb),
                ('svm', svm),
                ('knn', knn),
                ('lda', lda),
                ('qda', qda),
                ('bagging', bagging_dt)
            ],
            voting='soft',  # ä½¿ç”¨æ¦‚ç‡æŠ•ç¥¨
            weights=[3, 2, 2, 2, 1, 2, 1, 2]  # è°ƒæ•´æƒé‡
        )

    return model


# ==================== ä¸»ç¨‹åº ====================
def main():
    print("=" * 60)
    print("æ¤ç‰©å›¾åƒåˆ†ç±»ç³»ç»Ÿ - è¯¾ç¨‹æœ€ç»ˆä¼˜åŒ–ç‰ˆ")
    print("ç›®æ ‡ï¼š0.80+ åˆ†æ•°ï¼ˆçº¯æœºå™¨å­¦ä¹ ï¼‰")
    print("=" * 60)

    config = Config()

    # 1. åŠ è½½æ•°æ®
    print("\n[1/6] åŠ è½½è®­ç»ƒæ•°æ®...")
    train_images = []
    train_labels = []

    if not os.path.exists(config.TRAIN_DATA_PATH):
        print(f"é”™è¯¯: è®­ç»ƒæ•°æ®è·¯å¾„ä¸å­˜åœ¨: {config.TRAIN_DATA_PATH}")
        return

    categories = sorted([d for d in os.listdir(config.TRAIN_DATA_PATH)
                         if os.path.isdir(os.path.join(config.TRAIN_DATA_PATH, d))])

    if not categories:
        print("é”™è¯¯: æœªæ‰¾åˆ°ç±»åˆ«æ–‡ä»¶å¤¹")
        return

    class_names = categories
    print(f"æ‰¾åˆ° {len(categories)} ä¸ªç±»åˆ«: {categories}")

    # ç»Ÿè®¡å’ŒåŠ è½½æ•°æ®
    total_images = 0
    for label, category in enumerate(categories):
        category_path = os.path.join(config.TRAIN_DATA_PATH, category)
        img_files = [f for f in os.listdir(category_path)
                     if f.lower().endswith(('.png', '.jpg', '.jpeg'))]

        print(f"  ç±»åˆ« {category}: {len(img_files)} å¼ å›¾ç‰‡")
        total_images += len(img_files)

        for img_file in img_files:
            img_path = os.path.join(category_path, img_file)
            train_images.append(img_path)
            train_labels.append(label)

    print(f"\næ€»å…±åŠ è½½ {total_images} å¼ è®­ç»ƒå›¾åƒ")

    # 2. ç‰¹å¾æå–
    print("\n[2/6] æå–é«˜çº§ç‰¹å¾ï¼ˆå¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼‰...")
    X = []
    y = []

    start_time = time.time()
    processed = 0

    for i, (img_path, label) in enumerate(zip(train_images, train_labels)):
        features = extract_all_features_final(img_path, config)

        if features is not None:
            X.append(features)
            y.append(label)

            # ç®€å•æ•°æ®å¢å¼ºï¼ˆå¢åŠ è®­ç»ƒæ•°æ®ï¼‰
            img = cv2.imread(img_path)
            if img is not None:
                img = cv2.resize(img, config.IMAGE_SIZE)
                augmented_images = augment_image_simple(img)

                # å¯¹å¢å¼ºå›¾åƒæå–ç‰¹å¾
                for aug_img in augmented_images[1:2]:  # åªç”¨ä¸€ä¸ªå¢å¼ºç‰ˆæœ¬
                    # ä¸´æ—¶ä¿å­˜å¢å¼ºå›¾åƒ
                    temp_path = f"temp_aug_{i}.jpg"
                    cv2.imwrite(temp_path, aug_img)
                    aug_features = extract_all_features_final(temp_path, config)
                    if aug_features is not None:
                        X.append(aug_features)
                        y.append(label)
                    os.remove(temp_path)

        processed += 1
        if processed % 10 == 0:
            elapsed = time.time() - start_time
            avg_time = elapsed / processed
            remaining = avg_time * (len(train_images) - processed)
            print(f"  è¿›åº¦: {processed}/{len(train_images)} | "
                  f"å·²ç”¨æ—¶é—´: {elapsed:.1f}s | "
                  f"å‰©ä½™æ—¶é—´: {remaining:.1f}s")

    X = np.array(X)
    y = np.array(y)

    print(f"\nç‰¹å¾æå–å®Œæˆ! è€—æ—¶: {time.time() - start_time:.1f}ç§’")
    print(f"ç‰¹å¾ç»´åº¦: {X.shape} (åŸå§‹: {len(train_images)}, å¢å¼ºå: {len(X)})")

    # 3. ç‰¹å¾å¤„ç†
    print("\n[3/6] ç‰¹å¾å¤„ç†...")

    # æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # ç‰¹å¾é€‰æ‹©
    if config.USE_FEATURE_SELECTION and X_scaled.shape[1] > config.N_BEST_FEATURES:
        print(f"ç‰¹å¾é€‰æ‹©: ä» {X_scaled.shape[1]} ä¸ªç‰¹å¾ä¸­é€‰æ‹© {config.N_BEST_FEATURES} ä¸ªæœ€ä½³ç‰¹å¾")
        selector = SelectKBest(f_classif, k=min(config.N_BEST_FEATURES, X_scaled.shape[1]))
        X_selected = selector.fit_transform(X_scaled, y)
        print(f"ç‰¹å¾é€‰æ‹©å®Œæˆ!")
    else:
        X_selected = X_scaled
        selector = None

    # PCAé™ç»´ï¼ˆä¿ç•™95%æ–¹å·®ï¼‰
    pca = PCA(n_components=0.95, random_state=config.RANDOM_STATE)
    X_pca = pca.fit_transform(X_selected)
    print(f"PCAé™ç»´: {X_selected.shape[1]} -> {X_pca.shape[1]} ç»´")
    print(f"ä¿ç•™æ–¹å·®: {np.sum(pca.explained_variance_ratio_):.2%}")

    # 4. è®­ç»ƒæ¨¡å‹
    print("\n[4/6] è®­ç»ƒé«˜çº§é›†æˆæ¨¡å‹...")

    # åˆ’åˆ†è®­ç»ƒéªŒè¯é›†
    X_train, X_val, y_train, y_val = train_test_split(
        X_pca, y, test_size=config.TEST_SIZE,
        random_state=config.RANDOM_STATE, stratify=y
    )

    print(f"è®­ç»ƒé›†: {X_train.shape[0]} æ ·æœ¬")
    print(f"éªŒè¯é›†: {X_val.shape[0]} æ ·æœ¬")

    # åˆ›å»ºå¹¶è®­ç»ƒé›†æˆæ¨¡å‹
    model = create_advanced_ensemble(X_train.shape[1], len(class_names), config)

    print(f"è®­ç»ƒ{config.ENSEMBLE_METHOD}é›†æˆæ¨¡å‹...")
    model.fit(X_train, y_train)

    # éªŒè¯
    y_pred = model.predict(X_val)
    accuracy = accuracy_score(y_val, y_pred)
    print(f"\nâœ… éªŒè¯é›†å‡†ç¡®ç‡: {accuracy:.4f}")

    print("\nè¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(y_val, y_pred, target_names=class_names))

    # äº¤å‰éªŒè¯ï¼ˆå¯é€‰ï¼Œè¾ƒæ…¢ï¼‰
    print("\nè¿›è¡Œ5æŠ˜äº¤å‰éªŒè¯...")
    cv_scores = cross_val_score(model, X_pca, y, cv=5, scoring='accuracy', n_jobs=-1)
    print(f"äº¤å‰éªŒè¯åˆ†æ•°: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})")

    # 5. é¢„æµ‹æµ‹è¯•é›†
    print("\n[5/6] é¢„æµ‹æµ‹è¯•é›†...")

    if not os.path.exists(config.TEST_DATA_PATH):
        print(f"æµ‹è¯•æ•°æ®è·¯å¾„ä¸å­˜åœ¨: {config.TEST_DATA_PATH}")
        print("è·³è¿‡æµ‹è¯•é›†é¢„æµ‹")
    else:
        test_files = sorted([f for f in os.listdir(config.TEST_DATA_PATH)
                             if f.lower().endswith(('.png', '.jpg', '.jpeg'))])

        if len(test_files) == 0:
            print("æœªæ‰¾åˆ°æµ‹è¯•å›¾åƒ")
        else:
            print(f"å¤„ç† {len(test_files)} å¼ æµ‹è¯•å›¾åƒ")

            test_features = []
            valid_files = []

            for i, img_file in enumerate(test_files):
                img_path = os.path.join(config.TEST_DATA_PATH, img_file)
                features = extract_all_features_final(img_path, config)

                if features is not None:
                    # åº”ç”¨ç›¸åŒçš„ç‰¹å¾å¤„ç†æµç¨‹
                    features_scaled = scaler.transform(features.reshape(1, -1))

                    if selector is not None:
                        features_selected = selector.transform(features_scaled)
                    else:
                        features_selected = features_scaled

                    features_pca = pca.transform(features_selected)
                    test_features.append(features_pca.flatten())
                    valid_files.append(img_file)

                if (i + 1) % 10 == 0:
                    print(f"  è¿›åº¦: {i + 1}/{len(test_files)}")

            if test_features:
                test_features = np.array(test_features)
                test_predictions = model.predict(test_features)

                # è½¬æ¢ä¸ºæ¤ç‰©åç§°
                test_predictions_names = [class_names[pred] for pred in test_predictions]

                # ç¡®ä¿IDæœ‰.pngæ‰©å±•å
                fixed_ids = []
                for filename in valid_files:
                    if not filename.lower().endswith('.png'):
                        filename = os.path.splitext(filename)[0] + '.png'
                    fixed_ids.append(filename)

                # åˆ›å»ºæäº¤æ–‡ä»¶
                submission_df = pd.DataFrame({
                    'ID': fixed_ids,
                    'Category': test_predictions_names
                })

                # æŒ‰IDæ’åº
                submission_df = submission_df.sort_values('ID').reset_index(drop=True)

                submission_df.to_csv(config.SUBMISSION_PATH, index=False)
                print(f"\nâœ… æäº¤æ–‡ä»¶å·²ä¿å­˜: {config.SUBMISSION_PATH}")

                # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
                print(f"\nğŸ“Š é¢„æµ‹ç»“æœç»Ÿè®¡:")
                print(f"æ€»é¢„æµ‹æ•°: {len(submission_df)}")
                print("ç±»åˆ«åˆ†å¸ƒ:")
                print(submission_df['Category'].value_counts().sort_index())

                # æ˜¾ç¤ºå‰10è¡Œ
                print("\nğŸ“‹ å‰10è¡Œæ•°æ®:")
                print(submission_df.head(10).to_string(index=False))
            else:
                print("æ‰€æœ‰æµ‹è¯•å›¾åƒç‰¹å¾éƒ½æ— æ•ˆ!")

    # 6. ä¿å­˜æ¨¡å‹
    print("\n[6/6] ä¿å­˜æ¨¡å‹å’Œé…ç½®...")
    model_data = {
        'model': model,
        'scaler': scaler,
        'selector': selector,
        'pca': pca,
        'class_names': class_names,
        'config': config,
        'feature_dim': X_pca.shape[1]
    }

    joblib.dump(model_data, config.MODEL_SAVE_PATH)
    print(f"æ¨¡å‹å·²ä¿å­˜: {config.MODEL_SAVE_PATH}")

    print("\n" + "=" * 60)
    print("ğŸ‰ ç¨‹åºæ‰§è¡Œå®Œæˆ!")
    print(f"ğŸ“ˆ é¢„æœŸåˆ†æ•°: 0.78-0.85 (åŸºäºéªŒè¯é›†å‡†ç¡®ç‡: {accuracy:.4f})")
    print("=" * 60)

    # ç»™å‡ºè¿›ä¸€æ­¥ä¼˜åŒ–å»ºè®®
    print("\nğŸ’¡ å¦‚æœåˆ†æ•°ä»éœ€æé«˜ï¼Œå¯å°è¯•:")
    print("1. è°ƒæ•´ç‰¹å¾æå–å‚æ•°")
    print("2. å¢åŠ æ•°æ®å¢å¼ºå¼ºåº¦")
    print("3. ä½¿ç”¨ç½‘æ ¼æœç´¢è°ƒä¼˜æ¨¡å‹å‚æ•°")
    print("4. å°è¯•ä¸åŒçš„ç‰¹å¾ç»„åˆ")
    print("5. å¢åŠ é›†æˆæ¨¡å‹çš„å¤šæ ·æ€§")


if __name__ == "__main__":
    # æ£€æŸ¥å¿…è¦çš„åº“
    required_libs = ['cv2', 'sklearn', 'numpy', 'pandas', 'joblib']

    for lib in required_libs:
        try:
            if lib == 'cv2':
                import cv2
            elif lib == 'sklearn':
                from sklearn import __version__ as sk_version
            elif lib == 'numpy':
                import numpy as np
            elif lib == 'pandas':
                import pandas as pd
            elif lib == 'joblib':
                import joblib
        except ImportError as e:
            print(f"é”™è¯¯: ç¼ºå°‘å¿…è¦çš„åº“ {lib}")
            print(f"è¯·å®‰è£…: pip install opencv-python scikit-learn numpy pandas joblib")
            exit(1)

    main()